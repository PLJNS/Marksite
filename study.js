var page = "Discrete Structures II <small>Final Exam Study<\/small>\n======================================================\n\nReview Slides\n-------------\n\n### Counting\n\n#### Multiplication principle\n\n-   If we classify a set of objects by a sequences of decisions,\n\tthen the number of objects is equal to the number of decisions\n\tmultiplied from the first to the last.\n-   Mathematically, if $A$ and $B$ are finite sets, then,\n\t$$|A_1 \\times A_2 \\times ... \\times A_n| = |A_1| \\cdot |A_2| \\cdot .... \\cdot |A_n|$$\n\n#### Permutations\n\n-   The number of ways to put $n$ things in order:\n\t$$n(n - 1)(n - 2) ... 2 \\cdot 1 = n!$$\n-   The number of ways to put $k$-out-of-$n$ things in order:\n\t$$n(n-1)(n-2)...(n-k+1) = \\frac{n!}{(n - k)!}$$\n-   Ways to order $n$ objects, with $n_1$ alike, ..., $n_r$ alike:\n\t$$\\frac{n!}{n_1!...n_r!} = {n \\choose n_1, ..., n_r}$$\n\n#### Combinations\n\n-   Ways to choose $k$-out-of-$n$ things where order does not matter:\n\t$$\\frac{n!}{k!(n - k)!} = {n \\choose k}$$\n\n#### Partitions\n\n-   The number of ways to divide $n$ indistinguisable objects in $r$\n\tnon-empy piles:\n\t$${n - 1 \\choose r - 1}$$\n-   The number of ways to divide $n$ indistinguisable objects in $r$\n\tpossibly empty piles:\n\t$$n + r - 1 \\choose r - 1 $$\n\n#### Binomial Theorm\n\n$$(x + y)^n = \\sum_{k = 0}^n {n \\choose k} x^k y^{n - k}$$ \n\n#### Combinations Identities\n\n$$\\sum_{k = 0}^n {n \\choose k} = 2^n$$\n\n$${n \\choose k} = {n - 1 \\choose k} + {n - 1 \\choose k - 1}$$\n\n#### Multinomial theorem\n\n$$(x_1 + ... + x_r)^n =$$\n$$\\sum_{0 \\le n_1 \\le ... \\le n_r \\le n} {n \\choose n_1, ..., n_r}x_1^{n_1}...x_r^{n_r}$$\n\n-   Sum over all ways to express $n$ as sum of $r$ non-negative integers:\n\t$$0 \\le n_1 \\le ... \\le n_r \\le n : n_1 + ... + n_r = n$$\n\n### Probability\n\n#### Sample Spaces and Events\n\n-   Sample space is a set $S$.\n-   Events are a subset of $S$.\n-   Intersection of $E$ and $F$: \"Both $E$ and $F$ happen.\"\n-   Union of $E$ and $F$: \"Either $E$ or $F$ or both happen.\"\n-   Complement of $E$: \"$E$ does not happen.\"\n-   $E$ and $F$ are *mutually exclusive* if intersection of $E$ and $F$ is empty.\n\n#### Probability Measure\n\n-   A probability measure is a function $P$ that assigns number $P(E)$ to each event\n\tand satisfies:\n\t1.  $0 \\le P(E) \\le 1$\n\t2.  $P(S) = 1$\n\t3.  For mutually exclusive events $E_1, E_2$, ...,\n\t\t$$P\\left(\\bigcup_{i = 1}^\\infty E_i \\right) = \\sum_{i = 1}^\\infty P(E_i)$$\n\n#### Basic Identities\n\n$$P(\\emptyset) = 0$$\n$$P(E^c) = 1 - P(E)$$\n$$P(E \\cup F) = P(E) + P(F) - P(E \\cap F)$$\n$$P(E) = P(E \\cap F) + P(E \\cap F^c)$$\n\n-   If $E$ and $F$ are mutually exclusive, then:\n\t$$ P(E \\cup F) = P(E) + P(F)$$\n\n#### Uniform Probability\n\n-   If all outcomes are equally likely, then:\n\t$$P(E) = \\frac{|E|}{|S|}$$\n\n#### Inclusion\/Exclusion\n\n$$ P \\left( \\bigcup_{i = 1}^n E_i \\right) = $$\n$$ \\sum_{i = 1}^n P(E_i) - \\sum_{1 \\le i_1 \\le i_2 \\le n} P(E_{i_1} \\cap E_{i_2}$$\n$$ ... + (-1)^{r + 1} \\times \\sum_{1 \\lt i_1 \\lt ... \\lt i_r \\le n} P(E_{i-1} \\cap ... \\cap E_{i_r}) ...$$\n$$ ... + (-1)^{n + 1} \\times P(E_1 \\cap ... \\cap E_n)$$\n\n#### Conditional Probability\n\n-   The probability of $E$ given that $F$ happened:\n\t$$P(E|F) = \\frac{P(E \\cap F)}{P(F)}$$\n\n#### Bayes's Theorem\n\n-   Think $E$ is evidence and $F$ is outcome:\n\t$$P(F|E) = \\frac{P(E|F)P(F)}{P(E|F)P(F) + P(E|F^c)P(F^c)}$$\n\n#### Multiplication Rule for Conditional Probability\n\n$$P(E \\cap ... E_n) = P(E_1)P(E_2 | E_1)P(E_3 | E_1 \\cap E_2) ... $$\n$$ P(E_n | E_1 \\cap E_2 ... \\cap E_{n - 1})$$\n\n#### Independent Events\n\n-   $E$ and $F$ are indepedant if\n\t$$P(E \\cap F) = P(E) \\times P(F)$$\n-   Equivilently,\n\t$$P(E|F) = P(E)$$\n\t$$P(F) = 0$$\n\n#### Prosecutor's Fallacy\n\n-   In general,\n\t$$P(E|F) \\neq P(F|E)$$\n\n#### Random Variables\n\n##### Defintion\n\n-   **Formal version**: $X$ is a random variable (RV) means it is a function from\n\ta sample space to numbers.\n-   **Intuitive version**: $X$ is a way to randomly pick numbers (possibly not all\n\twith equal probability).\n\n##### Indicator RVs\n\n-   $X$ is an indicator RV for an event $A$ means:\n\t-   If $w \\in A$\n\t\t$$X(w) = 1$$\n\t-   If $w \\notin A$\n\t\t$$X(w) = 0$$\n\n-   Intuitively, $X = 1$ means \"$A$ happened\" and $X=0$ means \"$A$ didn't happen.\"\n\t\n> **You should be able to ...**\n>\n> \t-   Give a random variable that describes a given experiment\n>\t-   Switch between intuitive and formal versions of random variables.\n\n##### Range, Partition, and Frequency Function\n\n-   $Range(X)$ is every value that $X$ can take.\n-   Frequency function gives the probability that $X$ takes each value in range:\n    $$Range(X) = \\lbrace a_1, ..., a_k \\rbrace$$\n\t$$A_i = \\lbrace w \\in S : X(W) = a_i \\rbrace$$\n\t$$fx(a_i) = P(X = a_i) = P(A_i)$$\t\n\n> **You should be able to ...**\n>\n>\t-   Find the range, partition, frequency function of a RV formally and intuitively.\n\n### Expectation\n\n-   Weighted average of a RV:\n\t$$E(X) = \\sum_{w \\in S} X(w) \\times P(w)$$\n-   Often easier formula:\n\t$$E(X) = \\sum_{a_i \\in Range(X)} a_i \\times P(X = a_i) $$\n\n>  **You should be able to**\n>\n>  -   Compute the expectation of a random variable. Which formula is up to you.\n\n#### Linearity of Expectation\n\n-   If $X_1, ..., X_n$ are *any* RV, then:\n\t$$E(X_1 + ... + X_n) = E(X_1) + ... + E(X_n)$$\n\t$$E(aX = b) = aE(X) + b)$$\n\n#### Using Linearity of Expectation\n\n-   To find $E(X)$ for a complicated $X$:\n\t1.  Express $X = X_1 + ... + X_n$ for \"simpler\" $X_i$.\n\t2.  $E(X) = E(X_1) + ... + E(X_n)$\n\t3.  If the $X_i$ are indicators for event $E_i$ then $E(X_i) = P(E_i)$ which\n\t\tis often easier.\n\n> **You should be able to...**\n> \n> -  Apply linearity of expectation in this way.\n> -  Understand how to break a problem up to make it easier.\n> -  Recognize when you are falling into a trap by using the original expectation\n>    formual directly.\n\n#### Expectation of a function of a RV\n\n-   If $g$ is any function, then\n\t$$E(g(X)) = \\sum_{a_i \\in Range(X)} g(a_i) \\times P(X = a_i)$$\n-   In particular,\n\t$$E(X^2) = \\sum_{a_i \\in Range(X)} a_i^2 \\times P(X = a_i)$$\n\n### Variance\n\n-   Measure if \"swingyness\" of a RV:\n\t$$ V(X) = E((X - E(X))^2)$$\n-   Often easier to use:\n\t$$ V(X) = E(X^2) - E(X)^2$$\n-   Be careful when computers $E(X^2)$\n\n#### Variance formulas\n\n-   For any RV $X$ and number $c$:\n\t$$V(cX) = c^2 V(X)$$\n-   If $X$ and $Y$ are indepedant then:\n\t$$V(X + Y) = V(X) + V(Y)$$\n\n#### Covariance\n\n-   For any $X$ and $Y$,\n\t$$Cov(X,y) = E((X - E(X))(Y - E(Y)))$$\n-   Easier formula\n\t$$Cov(X,Y) = E(XY) - E(X)E(Y)$$\n\n> **You should be able to ...**\n>\n> -  Computer variance of a random variable.\n> -  Derive simple formula about variance.\n\n### Four Common Frequency Functions\n\n#### Bernoulli\n\n-   Indicator for success in one trial.\n-   If a RV $X$ has the Bernouilli frequency function with\n\tparameter $p$, then:\n\t$$Range(X) = \\lbrace 0, 1 \\rbrace$$\n\t$$P(X = 1) = fx(1) = p$$\n\t$$P(X = 0) = fx(0) = 1 - p$$\n\t$$E(X) = p$$\n\t$$V(X) = p(1 - p)$$\n\n\n#### Binomial\n\n-   The number of successes in $n$ trials.\n-   If a RV $X$ has the bionmial frequency function with parameters $n$ and $p$\n\tthen:\n\t$$Range(X) = \\lbrace 0, ... , n \\rbrace$$\n\t$$P(X = k) = fx(k) = {n \\choose k} p^k (1 - p)^k$$\n\t$$E(X) = pn $$\n\t$$V(X) = p(1 - p)n$$\n\n#### Geometric\n\n-   The number of trials until first success.\n-   If a RN $X$ has the geometric frequency function with parameter $p$, then:\n\t$$Range(X) = \\lbrace 0, 1, ... \\rbrace$$\n\t$$P(X = n) = fx(n) = p(1 - p)^{n - 1}$$\n\t$$E(X) = \\frac{1}{p}$$\n\t$$V(X) = \\frac{1 - p}{p^2}$$\n\n#### Negative Binomial\n\n-   The number of trials until %k%-th success\n-   If a RV $X$ has the negative binomial frequency function with parameters\n\t$k$ and $p$, then:\n\t$$Range(X) = \\lbrace 0, 1, ... \\rbrace$$\n\t$$P(X = n) = fx(n) = {n - 1 \\choose k - 1} p^k (1 - p)^{n - k}$$\n\t$$E(X) = \\frac{k}{p}$$\n\t$$V(X) = \\frac{k(1 - p)}{p^2}$$\n\n#### You should be able to...\n\n-   Recognize the frequency function of a random variable.\n-   Apply that knowledge to compute somehting about the random variable.\n\n### Two Inequalities\n\n#### Markov's Inequality\n\n-   If $X$ only takes non-negative values, then:\n\t$$ P(X \\ge t) \\le \\frac{E(X)}{t} $$\n\n#### Chebyshev's Inequality\n\n-   For any RV $X$,\n\t$$P(|X - E(X) | \\ge \\epsilon) \\le \\frac{V(X)}{\\epsilon^2}$$\n\n>  **You should be able to ...**\n>\n> - Recognize which inequality applies to a given situation.\n> - Apply the inequalities to given information.\n> - Derive simple consequeces (like homework)\n\n### Generating functions\n\n-   The generating function of a sequences $a_0, a_1, a_2, ...$ is:\n\t$$A(x) = \\sum_{n = 0}^\\infty a_n x^n$$\n-   We say \"$A$ generates $a_0, a_1, a_2, ...$\n\n#### Operations on Generating Functions\n\n-   Suppose $A(x) = \\sum_{n = 0}^\\infty a_n x^n$, then:\n\t-   $xA(x)$ generates $0, a_0, a_1, 2a_2 ...$\n\t-   $x^2 A(x)$ generates $0, 0, a_0, a_1, 2a_2 ...$\n\t-   $A'(x)$ generates $a_1, 2a_2, 3a_3, ...$\n\n#### Convolutions\n\n-   If $A(x)$ generates $\\lbrace a_i \\rbrace$ and $B(x)$ generates $\\lbrace b_i \\rbrace$\n\tthen $A(x)B(x)$ generates $\\lbrace c_i \\rbrace$:\n\t$$A(x)B(x) = C(x) = \\sum_{n = 0}^\\infty c_n x^n$$ \n\t-   Where:\n\t\t$$c_n = \\sum_{k = 0}^n a_k b_{n - k}$$\n\n> **You should be able to...**\n>\n> - Computee the closed form version of a generating function.\n> - Recognize functions that are in table, but some of the operations are applied. (tricky).\n\n#### Recurrence Relations\n\n-   Given a recurrence relations defining $a_n$ for all $n$, can find a formula for $a_n$\n-   Technique:\n\t1.  Let $A(x)$ be generating function for $\\lbrace a_n \\rbrace$\n\t2.  Find closed form of $A(x)$.\n\t3.  Use table to figure out series generated by $A(x)$.\n\n#### Probability Generating Functions\n\n-   For all RVs $X$ that take non-negative integer values, define:\n\t$$ G_x(z) = \\sum_{n = 0}^\\infty P(X = n) z^n$$\n\n\n";var converter = new Markdown.Converter();document.write(converter.makeHtml(page));
